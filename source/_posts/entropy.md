---
title: 信息熵的理解和应用
date: 2021-09-18 15:20:44
tags:
- 建模
- 技术
- 机器学习
categories: 机器学习
description: 重新学学基本数学概念
photos:
---

# 信息的定义  

信息是对事件不确定性的消除，不确定性越大，信息量越大。

概率与信息量关系：概率越大的事件，提供的信息量越小，概率越小的事件，信息量越大。

信息的度量在数学上的具体表达式应该是什么样子呢？

数学上一般是这么确认：

- 信息需要具备什么性质（特性）
- 尝试构造一个具备这些性质的表达式
- 证明这个表达式具备唯一性（反证法）

信息的性质：

- 设 $a_1,a_2$ 是两个随机事件，对应信息量 $f(a_1),f(a_2)$
- 若 $P(a_1)\leq P(a_2)$ ，则  $f(a_1) \geq f(a_2)$​​​
- $P(a_1)=0$ , $f(a_1)=\infty$
- $P(a_1)=1$，$f(a_1)=0$
- 若 $a_1，a_2$ 是独立事件，$f(a_1,a_2)=f(a_1)+f(a_2)$​

满足上诉性质的表达式是 
$$
I(a_i)=log\frac{1}{P(a_i)}
$$

> 以后看到表达式里面有 $log\frac1{P(a_i)}$就是代表事件 $a_i$ 的信息量

# 信息熵的定义和衍生量  

接下来引入随机变量(或者随机分布) $X$ 
$$
X={x_1,x_2,...,x_N},\\X具备N个状态\\
P(X)={p(x_1),p(x_2),...,p(x_N)}\\
P(x)就是X的随机分布
$$
$X$ 的平均度量，就是信息熵
$$
H(X)=E(log\frac1{P(X)})=\sum_{x \in X}p(x)log\frac1{p(x)}=-\sum_{x \in X}p(x)logp(x)
$$
熵的性质

-  $P(X)$ 为等概率分布时候，$H(X)$ 最大

**熵可以理解为是随机变量的信息度量**，在这个理解层面拓展，研究多个随机变量之间的关系，可以衍生出[联合熵、条件熵、互信息][]，它们之间的关系可以用集合的维恩图[^1]表示：
$$
I(X;Y)=H(X,Y)-H(X|Y)=H(X)+H(Y)-H(X,Y)
$$
**熵也可以理解是对一种随机分布的信息量的度量**，在这个理解层面上拓展，研究多个随机分布之间的关系，可以衍生出[交叉熵和K- L散度][]。

前面提及的范围都是对一种分布来讨论，下面讨论对同一个事件（具备N个状态）两种分布 $P={p_1,p_2,...,p_k,...,p_N}$  ，$Q={q_1,q_2,...,q_k,...,q_N}$ 

消除分布 $P$ 的不确定性需要最小的信息量（最优确认策略）是 $H(P)$ ， 那么，当我们使用非最优策略（根据分布 $Q$ 制定）消除系统的不确定性，所需要付出的信息量的大小（努力程度）我们该如何去衡量呢？

这就需要引入**交叉熵，其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要信息量（付出的努力的大小）**。

使用分布 $Q$制定策略，对于 $k$ 状态，需要的信息量为 $log\frac1{q_k}$，因此交叉熵为
$$
H(P||Q)=\sum_{k=1}^N{p_klog\frac1{q_k}}
$$
相对熵  $D(P||Q)$ 表示非最优策略和最优策略之间的差异，度量了从Q分布逼近P分布信息量差异（具有方向性）,
$$
\begin{align*}
&D(P||Q)= H(P||Q)-H(P)\\
&= \sum_{k=1}^N{p_klog\frac1{q_k}}-\sum_{k=1}^N{p_klog\frac1{p_k}}\\
&=\sum_{k=1}^N{p_klog\frac{p_k}{q_k}}
\end{align*}
$$
相对熵就是 $K-L$ 散度。

为了消除K-L散度的方向性，引入**K-L距离**，衡量两个分布之间的差异。
$$
\begin{align*}
&D(P||Q)+D(Q||P)\\
&=\sum_{k=1}^N{p_klog\frac{p_k}{q_k}}+\sum_{k=1}^N{q_klog\frac{q_k}{p_k}}\\
&=\sum_{k=1}^N(p_k-q_k)log\frac{p_k}{q_k}\\
&=\sum_{k=1}^N(p_k-q_k)(log{p_k}-log{q_k})
\end{align*}
$$

# K-L 距离的使用  

**相对熵与IV的关系**

将K-L距离中的两个随机分布p、q换为模型预测后得到的正负样本的概率密度函数f(p|B)和f(p|G)就是IV

**相对熵与PSI的关系**

PSI本质上是实际分布（A）与预期分布（E）的K-L散度的一个对称化操作





[联合熵、条件熵、互信息]: https://zhuanlan.zhihu.com/p/140376729
[交叉熵和K- L散度]: https://zhuanlan.zhihu.com/p/339084218
[如何通俗的解释交叉熵与相对熵]: https://www.zhihu.com/question/41252833/answer/195901726
[相对熵与IV、PSI的关系]: https://zhuanlan.zhihu.com/p/339084218



[^1]: 集合关系的示意图











